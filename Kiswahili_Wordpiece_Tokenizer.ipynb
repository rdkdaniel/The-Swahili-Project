{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/rdkdaniel/Toward-Understanding-Kiswa-Tokenizer-Build/blob/main/Kiswahili_Wordpiece_Tokenizer.ipynb",
      "authorship_tag": "ABX9TyP1S2CGnFsoP70jqtIBn6Vy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdkdaniel/The-Swahili-Project/blob/main/Kiswahili_Wordpiece_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kiswahili WordPiece Tokenizer**"
      ],
      "metadata": {
        "id": "aol1EEYt8KJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "*   Kiswahili has a different strcture to English or any other well-researched language in NLP. \n",
        "*   It is therefore, important to design a tokenizer from scratch for Kiswahili to meet its NLP needs.\n",
        "\n"
      ],
      "metadata": {
        "id": "XSsQF_cn8UAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0.0 Libraries**"
      ],
      "metadata": {
        "id": "dD0lQYSz80B0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qWynlGCx7PLX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "679ccb73-a2e3-491e-ae93-102045bfa630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 huggingface-hub-0.12.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQwcGkaJUDVK",
        "outputId": "a148e64e-d5ab-4ef1-9e18-b227a7711afa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7VXHeiT9Hb9",
        "outputId": "834da099-3382-4bfd-a1c9-c29bae7a3be1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.2 transformers-4.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "GKgGTGMO9PIu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.0 Loading the Datasets**"
      ],
      "metadata": {
        "id": "1xjAImXg9hcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(r'/content/drive/MyDrive/train.csv')\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsUmgPEO9llE",
        "outputId": "fb37f693-39d2-4152-8a48-6f0bf5db65d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            id                                            content category\n",
            "0       SW4670   Bodi ya Utalii Tanzania (TTB) imesema, itafan...   uchumi\n",
            "1      SW30826   PENDO FUNDISHA-MBEYA RAIS Dk. John Magufuri, ...  kitaifa\n",
            "2      SW29725  Mwandishi Wetu -Singida BENKI ya NMB imetoa ms...   uchumi\n",
            "3      SW20901   TIMU ya taifa ya Tanzania, Serengeti Boys jan...  michezo\n",
            "4      SW12560   Na AGATHA CHARLES – DAR ES SALAAM ALIYEKUWA K...  kitaifa\n",
            "...        ...                                                ...      ...\n",
            "23263  SW24920   Alitoa pongezi hizo alipozindua rasmi hatua y...   uchumi\n",
            "23264   SW4038   Na NORA DAMIAN-DAR ES SALAAM  TEKLA (si jina ...  kitaifa\n",
            "23265  SW16649   Mkuu wa Mkoa wa Njombe, Dk Rehema Nchimbi wak...   uchumi\n",
            "23266  SW23291   MABINGWA wa Ligi Kuu Soka Tanzania Bara, Simb...  michezo\n",
            "23267  SW11778   WIKI iliyopita, nilianza makala haya yanayole...  kitaifa\n",
            "\n",
            "[23268 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.0 Building The Tokenizer**"
      ],
      "metadata": {
        "id": "wQ9lHQ9S-yR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Process Used**\n",
        "\n",
        "Tokenization involves 5 steps:\n",
        "\n",
        "\n",
        "*   Normalization - text cleanup such as lowercasing, removing accents etc.\n",
        "*   Pre-tokenization - splitting the words.\n",
        "*   Model - the actual tokenization.\n",
        "*   Post-processing - Special tokens are added.\n",
        "*   Decoder - the final step, tokenized data converted to human-readable text.\n",
        "\n"
      ],
      "metadata": {
        "id": "5kzkUZRa-_Lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Libraries**"
      ],
      "metadata": {
        "id": "JKkLMBb3_9jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import models"
      ],
      "metadata": {
        "id": "fUELnnK5AB3J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))"
      ],
      "metadata": {
        "id": "ZcXB-TxcAJvZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Normalization**"
      ],
      "metadata": {
        "id": "zta24m1sANYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import normalizers\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.Lowercase(), normalizers.NFKD()]\n",
        ")"
      ],
      "metadata": {
        "id": "oi9RskzNAPEE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 Pre-Tokenization**"
      ],
      "metadata": {
        "id": "AmA5vb7HATkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import pre_tokenizers\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace() # training from scratch or use  prebuilt BertPreTokenizer for prettained one."
      ],
      "metadata": {
        "id": "nkDqg5G-AXsp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4 Training the Tokenizer**"
      ],
      "metadata": {
        "id": "w0E15DufA56Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import trainers\n",
        "\n",
        "trainer = trainers.WordPieceTrainer(\n",
        "    vocab_size=30_000,\n",
        "    special_tokens=['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]'],\n",
        "    min_frequency=2,\n",
        "    continuing_subword_prefix='##'\n",
        ")"
      ],
      "metadata": {
        "id": "Ecc3zq5xA7di"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train_from_iterator(df, trainer=trainer)"
      ],
      "metadata": {
        "id": "4PEUeuGABElQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.5 Post Processing**"
      ],
      "metadata": {
        "id": "kKEQBnndBLP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import processors\n",
        "\n",
        "# first we get the token ID values (defined in the vocab) for CLS and SEP\n",
        "cls_id = tokenizer.token_to_id('[CLS]')\n",
        "sep_id = tokenizer.token_to_id('[SEP]')\n",
        "\n",
        "# then setup the post processing step with TemplateProcessing\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f'[CLS]:0 $A:0 [SEP]:0',\n",
        "    pair=f'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1',\n",
        "    special_tokens=[\n",
        "        ('[CLS]', cls_id),\n",
        "        ('[SEP]', sep_id)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "UmC5JgL6BNE9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.6 Decoder**"
      ],
      "metadata": {
        "id": "0cGVSrHBBQbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import decoders\n",
        "\n",
        "tokenizer.decoder = decoders.WordPiece(prefix='##')"
      ],
      "metadata": {
        "id": "MVIuTDkDBUKK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.7 Saving the Tokenizer**"
      ],
      "metadata": {
        "id": "-2TKzCu_BcGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# load the tokenizer in a transformers tokenizer instance\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    unk_token='[UNK]',\n",
        "    pad_token='[PAD]',\n",
        "    cls_token='[CLS]',\n",
        "    sep_token='[SEP]',\n",
        "    mask_token='[MASK]'\n",
        ")\n",
        "\n",
        "# save the tokenizer\n",
        "tokenizer.save_pretrained('/content/Kisw_wordpiece_tokn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNV83a-WBd64",
        "outputId": "b9dd4e4d-c52c-4bde-ef35-70250687d366"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/Kisw_wordpiece_tokn/tokenizer_config.json',\n",
              " '/content/Kisw_wordpiece_tokn/special_tokens_map.json',\n",
              " '/content/Kisw_wordpiece_tokn/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.0 Using the Tokenizer**"
      ],
      "metadata": {
        "id": "en1s_vXsB9Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('/content/Kisw_wordpiece_tokn')"
      ],
      "metadata": {
        "id": "u48I6ArzCAH1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(\"Ilikuwa wakati wa jioni jua limepunguza udhia wake na upepo mwanana ulikuwa ukipita na kuzipapasa ngozi zetu mfano wa pamba\")"
      ],
      "metadata": {
        "id": "dWTZ-d-XCGeW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOc1NzOTCKFd",
        "outputId": "69759851-cf20-4639-ef91-19c4e1dd40a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [2, 0, 0, 0, 0, 0, 0, 0, 0, 11, 21, 0, 0, 0, 0, 11, 21, 0, 0, 0, 0, 0, 0, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAfzNZ6hCa7Z",
        "outputId": "00d89399-c49c-4267-a9d1-c123b8b0d61e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PreTrainedTokenizerFast(name_or_path='/content/Kisw_wordpiece_tokn', vocab_size=26, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens.input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9QriBvdCiAt",
        "outputId": "3c0e68d0-0b7b-46ae-f329-ba66817b6619"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 0, 0, 0, 0, 0, 0, 0, 0, 11, 21, 0, 0, 0, 0, 11, 21, 0, 0, 0, 0, 0, 0, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref: https://huggingface.co/course/chapter6/8?fw=pt"
      ],
      "metadata": {
        "id": "cCKluyIWJjNW"
      }
    }
  ]
}