{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/rdkdaniel/The-Swahili-Project/blob/main/The_Tokenizer.ipynb",
      "authorship_tag": "ABX9TyN4UaY2+EA43njv1KG1c0G8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rdkdaniel/The-Swahili-Project/blob/main/The_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Kiswahili Under a Natural Language Processing Lens**"
      ],
      "metadata": {
        "id": "bQtY41sIa10T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "\n",
        "*   This notebook shows the process used to design the tokenizer for the Kiswahili Project (title above).\n",
        "*   Kiswahili is a low resource language but above that, it has a different morphological strcture than English or other languages whose tokenizers are readily available. \n",
        "*   It is therefore important to design a tokenizer specific to Kiswahili i.e. based on its strcture.\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ycPjCEU6FRiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Kiswahili Words and Sentences**\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "6G3e7XE3E9N5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.0 Libraries**"
      ],
      "metadata": {
        "id": "jUOAXxeXE3gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J70dK6h6FUyq",
        "outputId": "1796e624-2e3f-4f99-e103-f7f9d2c388e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
            "\u001b[K     |████████████████████████████████| 441 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 59.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 48.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting dill<0.3.6\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.6\n",
            "    Uninstalling dill-0.3.6:\n",
            "      Successfully uninstalled dill-0.3.6\n",
            "Successfully installed datasets-2.6.1 dill-0.3.5.1 huggingface-hub-0.10.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xlzZBH6tazHM"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87s7W_FWbIUI",
        "outputId": "4f1649d9-1a66-4223-9110-b4e3d6d3c473"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 29.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "BUxT8eEQbWIL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.0 Loading the Datasets**"
      ],
      "metadata": {
        "id": "WMB2zJ9aFZwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqKnrAFXevQ7",
        "outputId": "e0f76b83-f516-45ea-eed8-e1c13b58bade"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_fwf('/content/drive/MyDrive/Kiswahili_Dataset/Kiswahili_data1.txt')\n",
        "df2 = pd.read_fwf('/content/drive/MyDrive/Kiswahili_Dataset/Kiswahili_data2.txt')\n",
        "df3 = pd.read_fwf('/content/drive/MyDrive/Kiswahili_Dataset/Kiswahili_data3.txt')\n",
        "df4 = pd.read_fwf('/content/drive/MyDrive/Kiswahili_Dataset/Kiswahili_data4.txt')\n",
        "df5 = pd.read_fwf('/content/drive/MyDrive/Kiswahili_Dataset/Kiswahili_data5.txt')"
      ],
      "metadata": {
        "id": "HFEecHn7FfXg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df, df2, df3, df4, df5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjbbbkw_buMf",
        "outputId": "9a5a0f0c-cc18-4fd7-eb56-e7bc5074538b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Mhadhiri Denis Skopin (kushoto)  akiwa ameshikilia karatasi zake za  \\\n",
            "0  Dikteta wa Soviet Joseph Stalin  amepitia aina  fulani ya ukarabati   \n",
            "\n",
            "   kufukuzwa kutoka Chuo  Kikuu cha  \\\n",
            "0  katika Urusi ya Putin  - unaweza   \n",
            "\n",
            "   Jimbo la St PetersburgKatika nyumba yake ya St  \\\n",
            "0  hata kununua bidhaa za Stalin.Mhadhiri wa chuo   \n",
            "\n",
            "   Petersburg, mhadhiri wa chuo kikuu  \\\n",
            "0  kikuu aliyefutwa kazi Denis Skopin   \n",
            "\n",
            "  Denis Skopin ananionyesha hati ambayo imebadilisha maisha  \\\n",
            "0  amesoma miaka ya Stalin. Anaona uwiano kati ya...          \n",
            "\n",
            "  yake.Maelekezo: \"Maelekezo No.87/2D. Kuhusu: Kufutwa kazi.\"Hadi  hivi  \\\n",
            "0  na sasa.\"Nimetoka kuchapisha kitabu kwa Kiinge...               watu   \n",
            "\n",
            "   majuzi Denis  ... kusahau madoa ya.23 umwagaji damu ya.24 historia ya.25  \\\n",
            "0  wa  Urusi ya  ...     NaN   NaN   NaN      NaN  NaN   NaN      NaN   NaN   \n",
            "\n",
            "  nchi.1 yetu.\"  \n",
            "0    NaN    NaN  \n",
            "\n",
            "[1 rows x 402 columns]    KNEC STUDY MATERIALS, REVISION KITS  AND PAST PAPERSSTUDY FOR  \\\n",
            "0  Δdocument.getElementById( \"ak_js_1\"  ).setAttribute( \"value\",   \n",
            "\n",
            "   FREEKenya Certificate of  Secondary  EducationKiswahili Karatasi ya  \\\n",
            "0  ( new Date() ).getTime()  );We’re a  team of professionals who have   \n",
            "\n",
            "   21. UFAHAMUSoma kifungu kifuatacho  kisha ujibu  maswali. (Alama  \\\n",
            "0  taught in several schools and been  involved in  marking of KCPE   \n",
            "\n",
            "   15)Alikuwa wakati wa  jioni jua limepunguza udhia  ... website  in this  \\\n",
            "0  and KCSE exams. This  website has curated content  ...     NaN NaN  NaN   \n",
            "\n",
            "  browser  for  the  next  time   I  comment.  \n",
            "0     NaN  NaN  NaN   NaN   NaN NaN       NaN  \n",
            "\n",
            "[1 rows x 1886 columns]    KNEC STUDY MATERIALS, REVISION KITS  AND PAST PAPERSSTUDY FOR  \\\n",
            "0  Δdocument.getElementById( \"ak_js_1\"  ).setAttribute( \"value\",   \n",
            "\n",
            "   FREEKenya Certificate of  Secondary  \\\n",
            "0  ( new Date() ).getTime()  );We’re a   \n",
            "\n",
            "  Education Kiswahili Karatasi ya 3SEHEMU A : TAMTHILIAP.Kea:Kigogo1. Lazima(a)  \\\n",
            "0  team of professionals who have taught in sever...                              \n",
            "\n",
            "  “Aikose? Asiya ni mwanamke halisi bwana! Amemweka Bi Husda hapa.” (i) Eleza muktadha wa  \\\n",
            "0  marking of KCPE and KCSE exams. This website h...                                        \n",
            "\n",
            "  dondoo hili. (alama 10)(ii) Eleza vipengele viwili vya kimtindo  \\\n",
            "0  simplified study materials and revision papers...   your exams   \n",
            "\n",
            "   alivyotumia  msemaji  ...  website  in  this  browser  for  the  next  \\\n",
            "0          NaN      NaN  ...      NaN NaN   NaN      NaN  NaN  NaN   NaN   \n",
            "\n",
            "   time   I  comment.  \n",
            "0   NaN NaN       NaN  \n",
            "\n",
            "[1 rows x 1145 columns]    KNEC STUDY MATERIALS, REVISION KITS  AND PAST PAPERSSTUDY FOR  \\\n",
            "0  Δdocument.getElementById( \"ak_js_1\"  ).setAttribute( \"value\",   \n",
            "\n",
            "   FREEKenya Certificate of  Secondary  \\\n",
            "0  ( new Date() ).getTime()  );We’re a   \n",
            "\n",
            "  Education KiswahiliUFAHAMU (Alama 15)Soma kifungu kifuatacho  \\\n",
            "0  team of professionals who have taught in sever...             \n",
            "\n",
            "   kisha ujibu maswali.“Mabibi  \\\n",
            "0  been involved in marking of   \n",
            "\n",
            "  na mabwana, ndugu wapenzi, alianza Bi. Mkesha, “huu ni mwaka wa  \\\n",
            "0  KCPE and KCSE exams. This website has curated ...                \n",
            "\n",
            "  tano tangu tufanye uchaguzi mkuu uliowapa vigoda wenzetu hawa. Katika kipindi  \\\n",
            "0  you the best simplified study materials and re...                              \n",
            "\n",
            "   hiki  kumetokea  ...  website  in  this  browser  for  the  next  time   I  \\\n",
            "0   NaN        NaN  ...      NaN NaN   NaN      NaN  NaN  NaN   NaN   NaN NaN   \n",
            "\n",
            "   comment.  \n",
            "0       NaN  \n",
            "\n",
            "[1 rows x 1579 columns]    KNEC STUDY MATERIALS, REVISION KITS  AND PAST PAPERSSTUDY FOR  \\\n",
            "0  Δdocument.getElementById( \"ak_js_1\"  ).setAttribute( \"value\",   \n",
            "\n",
            "   FREEKenya Certificate of  Secondary  Education Kiswahili –  \\\n",
            "0  ( new Date() ).getTime()  );We’re a  team of professionals   \n",
            "\n",
            "   Fasihi Karatasi  ya  \\\n",
            "0  who have taught  in   \n",
            "\n",
            "  31.LazimaN ilreyaandika lâaaneno haya KWa lıiaba ya:Malrilioni wasio  \\\n",
            "0  several schools and been involved in marking o...                     \n",
            "\n",
            "   malaziWabebao vifurushi vilivyo waziwazungukao  \\\n",
            "0  This website has curated content by giving you   \n",
            "\n",
            "   barabaranı bil a mavazi Milki yao ya ıraisha.Kwa  ... website  in  this  \\\n",
            "0  the best simplified study materials and revision  ...     NaN NaN   NaN   \n",
            "\n",
            "   browser  for  the  next  time   I  comment.  \n",
            "0      NaN  NaN  NaN   NaN   NaN NaN       NaN  \n",
            "\n",
            "[1 rows x 909 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Wapi makofi ya good data scrapped na mimi!!\n",
        "*   👏 👏\n",
        "\n"
      ],
      "metadata": {
        "id": "_wzMI65_bzge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Merge the DF**"
      ],
      "metadata": {
        "id": "CwgTxp_rgVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.concat([df, df2, df3, df4, df5])\n",
        "#df_merged = pd.merge(df, df2, df3, df4, df5)"
      ],
      "metadata": {
        "id": "GDHs_Q8ggdZx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_merged)"
      ],
      "metadata": {
        "id": "RkFtmkRph9ch",
        "outputId": "5ebd9c94-c89e-4b28-b98b-3675f7a89634",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Mhadhiri Denis Skopin (kushoto)  akiwa ameshikilia karatasi zake za  \\\n",
            "0  Dikteta wa Soviet Joseph Stalin  amepitia aina  fulani ya ukarabati   \n",
            "0                              NaN                                 NaN   \n",
            "0                              NaN                                 NaN   \n",
            "0                              NaN                                 NaN   \n",
            "0                              NaN                                 NaN   \n",
            "\n",
            "   kufukuzwa kutoka Chuo  Kikuu cha  \\\n",
            "0  katika Urusi ya Putin  - unaweza   \n",
            "0                    NaN        NaN   \n",
            "0                    NaN        NaN   \n",
            "0                    NaN        NaN   \n",
            "0                    NaN        NaN   \n",
            "\n",
            "   Jimbo la St PetersburgKatika nyumba yake ya St  \\\n",
            "0  hata kununua bidhaa za Stalin.Mhadhiri wa chuo   \n",
            "0                                             NaN   \n",
            "0                                             NaN   \n",
            "0                                             NaN   \n",
            "0                                             NaN   \n",
            "\n",
            "   Petersburg, mhadhiri wa chuo kikuu  \\\n",
            "0  kikuu aliyefutwa kazi Denis Skopin   \n",
            "0                                 NaN   \n",
            "0                                 NaN   \n",
            "0                                 NaN   \n",
            "0                                 NaN   \n",
            "\n",
            "  Denis Skopin ananionyesha hati ambayo imebadilisha maisha  \\\n",
            "0  amesoma miaka ya Stalin. Anaona uwiano kati ya...          \n",
            "0                                                NaN          \n",
            "0                                                NaN          \n",
            "0                                                NaN          \n",
            "0                                                NaN          \n",
            "\n",
            "  yake.Maelekezo: \"Maelekezo No.87/2D. Kuhusu: Kufutwa kazi.\"Hadi  hivi  \\\n",
            "0  na sasa.\"Nimetoka kuchapisha kitabu kwa Kiinge...               watu   \n",
            "0                                                NaN                NaN   \n",
            "0                                                NaN                NaN   \n",
            "0                                                NaN                NaN   \n",
            "0                                                NaN                NaN   \n",
            "\n",
            "   majuzi Denis  ... kuchagua mbinu.4 hii..3 6)(b).1 sita.2 kutumia.1  \\\n",
            "0  wa  Urusi ya  ...      NaN     NaN    NaN     NaN    NaN       NaN   \n",
            "0           NaN  ...      NaN     NaN    NaN     NaN    NaN       NaN   \n",
            "0           NaN  ...      NaN     NaN    NaN     NaN    NaN       NaN   \n",
            "0           NaN  ...      NaN     NaN    NaN     NaN    NaN       NaN   \n",
            "0           NaN  ...      NaN     NaN    NaN     NaN    NaN       NaN   \n",
            "\n",
            "  kuUdumisha utanzu semi. 6)Your  \n",
            "0        NaN    NaN   NaN    NaN  \n",
            "0        NaN    NaN   NaN    NaN  \n",
            "0        NaN    NaN   NaN    NaN  \n",
            "0        NaN    NaN   NaN    NaN  \n",
            "0        NaN    NaN   NaN    NaN  \n",
            "\n",
            "[5 rows x 4347 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.0 Building The Tokenizer**"
      ],
      "metadata": {
        "id": "tuOnmNJaHayo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Brief Overview of the Process**\n",
        "\n",
        "Tokenization involves several steps:\n",
        "\n",
        "1.   Normalization - which involves text cleanup such as lowercasing, removing accents or weird characters with Unicode normalization, etc\n",
        "2.   Pre-tokenization - splitting the words into parts.\n",
        "3.   Model - the actual tokenization where characters or subwords are merged into logical components.\n",
        "4.   Post-processing - at thsis step, special tokens are added and these tokens are translated into IDs.\n",
        "5.   Decoder - the final step that takes the tokenized data and converts it into human-readable text. Often this step is not seen as part of the tokenization process but is necessary to understand any text-based model output.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mS3T_5MBHnJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Libraries**"
      ],
      "metadata": {
        "id": "SbMsFI_nJVuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import models"
      ],
      "metadata": {
        "id": "j83fSwweHfxC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.WordPiece(unk_token='[UNK]'))"
      ],
      "metadata": {
        "id": "ziuwZqZQckDn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Normalization**"
      ],
      "metadata": {
        "id": "dBU-cZ1wKiUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import normalizers\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.Lowercase(), normalizers.NFKD()]\n",
        ")"
      ],
      "metadata": {
        "id": "NC493aZtKkBX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.3 Pre-Tokenization**"
      ],
      "metadata": {
        "id": "OjlbGTxhKqrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import pre_tokenizers\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ],
      "metadata": {
        "id": "90oXds2GKsgr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.4 Training the Tokenizer**"
      ],
      "metadata": {
        "id": "OB32uJHOKwKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import trainers\n",
        "\n",
        "trainer = trainers.WordPieceTrainer(\n",
        "    vocab_size=30_000,\n",
        "    special_tokens=['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]'],\n",
        "    min_frequency=2,\n",
        "    continuing_subword_prefix='##'\n",
        ")"
      ],
      "metadata": {
        "id": "Orn-14oiK0aX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train_from_iterator(df_merged, trainer=trainer)"
      ],
      "metadata": {
        "id": "QUbPOIA7K7gx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.5 Post Processing**"
      ],
      "metadata": {
        "id": "IDRSuJ8gLb9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import processors\n",
        "\n",
        "# first we get the token ID values (defined in the vocab) for CLS and SEP\n",
        "cls_id = tokenizer.token_to_id('[CLS]')\n",
        "sep_id = tokenizer.token_to_id('[SEP]')\n",
        "\n",
        "# then setup the post processing step with TemplateProcessing\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f'[CLS]:0 $A:0 [SEP]:0',\n",
        "    pair=f'[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1',\n",
        "    special_tokens=[\n",
        "        ('[CLS]', cls_id),\n",
        "        ('[SEP]', sep_id)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "0pUmfKxgK7qo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.6 Decoder**"
      ],
      "metadata": {
        "id": "wIntyjdBLjXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import decoders\n",
        "\n",
        "tokenizer.decoder = decoders.WordPiece(prefix='##')"
      ],
      "metadata": {
        "id": "xix0xVrDLnNE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.0 Saving the Tokenizer**"
      ],
      "metadata": {
        "id": "xF6ibJJLLn4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# load the tokenizer in a transformers tokenizer instance\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    unk_token='[UNK]',\n",
        "    pad_token='[PAD]',\n",
        "    cls_token='[CLS]',\n",
        "    sep_token='[SEP]',\n",
        "    mask_token='[MASK]'\n",
        ")\n",
        "\n",
        "# save the tokenizer\n",
        "tokenizer.save_pretrained('RDK-Kisw-Tokenizer')"
      ],
      "metadata": {
        "id": "xccwsNnwL5Oi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd777e5e-98df-45a9-ebfa-61d5b3719667"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('RDK-Kisw-Tokenizer/tokenizer_config.json',\n",
              " 'RDK-Kisw-Tokenizer/special_tokens_map.json',\n",
              " 'RDK-Kisw-Tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.0 Using the Tokenizer**"
      ],
      "metadata": {
        "id": "VvmuoFuqL_KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained('RDK-Kisw-Tokenizer')"
      ],
      "metadata": {
        "id": "LTa-mLjvMClk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Ilikuwa wakati wa jioni jua limepunguza udhia wake na upepo mwanana ulikuwa ukipita na kuzipapasa ngozi zetu mfano wa pamba\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVlt3ckmdiq-",
        "outputId": "60310d72-2ace-4bd0-9472-7a696fc9262a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [2, 1345, 634, 135, 1951, 1133, 1055, 770, 84, 161, 164, 52, 268, 79, 433, 153, 1442, 1937, 1363, 2011, 153, 2066, 2110, 663, 590, 135, 537, 141, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}